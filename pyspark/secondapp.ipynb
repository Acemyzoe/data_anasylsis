{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Pyspark 广播和累加器\n",
    "对于并行处理，Apache Spark使用共享变量。当驱动程序将任务发送到集群上的执行程序时，共享变量的副本将在集群的每个节点上运行，以便可以将其用于执行任务。  \n",
    "Apache Spark支持两种类型的共享变量:  \n",
    "Broadcast  \n",
    "Accumulato"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## 广播\n",
    "Broadcast 广播变量用于跨所有节点保存数据副本。此变量缓存在所有计算机上，而不是在具有任务的计算机上发送。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['scala', 'java', 'python', 'spark']\npython\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext(\"local\",\"broadcast app\")\n",
    "word = sc.broadcast([\"scala\",\"java\",\"python\",\"spark\"])\n",
    "print(word.value) # Broadcast变量有一个名为value的属性，它存储数据并用于返回广播值。\n",
    "print(word.value[2])"
   ]
  },
  {
   "source": [
    "## 累加器\n",
    "Accumulate 累加器变量用于通过关联和交换操作聚合信息。例如，您可以使用累加器进行求和操作或计数器（在MapReduce中）。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "150\n[20, 30, 40, 50]\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "# sc = SparkContext(\"local\",\"Accumulator app\")\n",
    "num = sc.accumulator(10)\n",
    "def f(x):\n",
    "   global num\n",
    "   num+=x\n",
    "rdd = sc.parallelize([20,30,40,50])\n",
    "rdd.foreach(f)\n",
    "print(num.value)\n",
    "print(rdd.collect())"
   ]
  },
  {
   "source": [
    "# PySpark SparkConf\n",
    "\n",
    "以下是SparkConf最常用的一些属性\n",
    "\n",
    "set（key，value） - 设置配置属性。\n",
    "\n",
    "setMaster（value） - 设置主URL。\n",
    "\n",
    "setAppName（value） - 设置应用程序名称。\n",
    "\n",
    "get（key，defaultValue = None） - 获取密钥的配置值。\n",
    "\n",
    "setSparkHome（value） - 在工作节点上设置Spark安装路径。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "conf = SparkConf().setAppName(\"PySpark App\").setMaster(\"local\")\n",
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "source": [
    "# PySpark SparkFiles\n",
    "在Apache Spark中，您可以使用 sc.addFile 上传文件（sc是您的默认SparkContext），并使用 SparkFiles.get 获取工作者的路径。  \n",
    "SparkFiles解析通过 SparkContext.addFile（） 添加的文件的路径。  \n",
    "SparkFiles包含以下类方法:  \n",
    "get(filename)  \n",
    "getrootdirectory()"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "/tmp/spark-d4ba64bd-845f-400f-81a4-2f9086435ecb/userFiles-629fa65e-3b3d-47e7-a601-b13ea7e71e55/500_points.txt\n/tmp/spark-d4ba64bd-845f-400f-81a4-2f9086435ecb/userFiles-629fa65e-3b3d-47e7-a601-b13ea7e71e55\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkFiles\n",
    "path = \"/home/ace/Desktop/github/data_anasylsis/pyspark/5000_points.txt\"\n",
    "pathname = \"500_points.txt\"\n",
    "sc.addFile(path)\n",
    "print(SparkFiles.get(pathname))\n",
    "print(SparkFiles.getRootDirectory())"
   ]
  },
  {
   "source": [
    "# PySpark MLlib\n",
    "mllib.classification - 支持二进制分类，多类分类和回归分析的各种方法。分类中一些最流行的算法是 随机森林，朴素贝叶斯，决策树 等。\n",
    "\n",
    "mllib.clustering - 聚类是一种无监督的学习问题，您可以根据某些相似概念将实体的子集彼此分组。\n",
    "\n",
    "mllib.fpm - 频繁模式匹配是挖掘频繁项，项集，子序列或其他子结构，这些通常是分析大规模数据集的第一步。 多年来，这一直是数据挖掘领域的一个活跃的研究课题。\n",
    "\n",
    "mllib.linalg - 线性代数的MLlib实用程序。\n",
    "\n",
    "mllib.recommendation - 协同过滤通常用于推荐系统。 这些技术旨在填写用户项关联矩阵的缺失条目。它目前支持基于模型的协同过滤，其中用户和产品由一小组可用于预测缺失条目的潜在因素描述。 spark.mllib使用交替最小二乘（ALS）算法来学习这些潜在因素。\n",
    "\n",
    "mllib.regression - 线性回归属于回归算法族。 回归的目标是找到变量之间的关系和依赖关系。使用线性回归模型和模型摘要的界面类似于逻辑回归案例。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# PySpark Serializers\n",
    "序列化用于Apache Spark的性能调优。通过网络发送或写入磁盘或持久存储在内存中的所有数据都应序列化。  \n",
    " \n",
    "Marshal Serializer  此序列化程序比PickleSerializer更快，但支持更少的数据类型。  \n",
    "Pickle Serializer   此序列化程序几乎支持任何Python对象，但可能不如更专业的序列化程序快"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[0, 2, 4, 6, 8, 10, 12, 14, 16, 18]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.context import SparkContext\n",
    "from pyspark.serializers import MarshalSerializer\n",
    "sc.stop()\n",
    "sc = SparkContext(\"local\", \"serialization app\", serializer = MarshalSerializer()) # 使用MarshalSerializer序列化数据\n",
    "print(sc.parallelize(list(range(1000))).map(lambda x: 2 * x).take(10))\n",
    "sc.stop()"
   ]
  }
 ]
}